{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a198c84-19a6-410e-bff0-49c4ccfe0549",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vanilla Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) excel at tasks like image classification, where fixed-size inputs correspond to fixed-size outputs. However, they face challenges with variable-length sequences, such as time series, text sequences, and image sequences. Recurrent Neural Networks (RNNs) come to the forefront as a solution for processing sequential data.\n",
    "\n",
    "RNNs find application in diverse fields such as speech recognition, music generation, sentiment analysis, video processing, and text analysis and translation. Their ability to handle sequences makes them a powerful tool in capturing temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e36b4-51a4-4b45-94e9-03fc10bfaa15",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The term **Vanilla RNN** is often used to refer to the basic form of recurrent neural network with a single hidden layer and without architectural enhancements. Vanilla RNN has a simple architecture consisting of an **input layer**, a recurrent **hidden layer** and an **output layer**.\n",
    "\n",
    "A basic RNN processes a time series of input data $\\boldsymbol X$ by estimating the output $\\boldsymbol y_t$ given the input vector $\\boldsymbol x_t$ and the hidden state vector $\\boldsymbol h_t$. The hidden state is updated at each time step. It acts as a memory of previous time steps allowing the network to capture sequential patterns.\n",
    "\n",
    "![RNN structure](images/rnn.png)\n",
    "\n",
    "For instance, consider a natural language processing task where $\\boldsymbol X$ is a sequence of words in a sentence, $\\boldsymbol x_t$ is the word at position $\\boldsymbol t$, and $\\boldsymbol y_t$ represents the predicted probability distribution over the vocabulary for the next word in the sequence. RNN learns from the context of previous words, using the hidden state to generate predictions for the next word in the sentence.\n",
    "\n",
    "````{important}\n",
    "While we focus on many-to-many paradigm, where sequences are processed, and outputs are generated at each time step, it is important to highlight flexivbility of RNNs to different input-output models. RNNs can process different sequence-to-sequence architectures, such as one-to-one, one-to-many, many-to-one. \n",
    "\n",
    "````{admonition} What are examples of applications that align with these models?\n",
    ":class: dropdown\n",
    "![RNN flexibility](images/flexibility.png)\n",
    "\n",
    "**Q.** What sequence processing model do you think is suitable for sentiment analysis tasks?\n",
    "````\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021e899-0368-4438-b81c-27cb9ac82304",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "![RNN forward](images/forward.png)\n",
    "\n",
    "Consider a minibatch of inputs $\\boldsymbol x_t$ $\\in\\mathbb{R}^{n \\times d}$ at time step $\\boldsymbol t$. Each row of $\\boldsymbol x_t$ corresponds to one example at time step $\\boldsymbol t$ within a minibatch of $n$ sequence examples. The weight parameter $\\boldsymbol W_{xh}$ $\\in \\mathbb{R}^{d \\times h}$ and bias parameter $\\boldsymbol b_h$ are applied to the current input. Additionally, let $\\boldsymbol h_t$ $\\in \\mathbb{R}^{n \\times h}$ denote the hidden layer output at time step $\\boldsymbol t$. The calculation of the hidden layer output, $\\boldsymbol h_t$ at the current time step, $\\boldsymbol t$ is determined by:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol h_t = \\phi({\\boldsymbol x_tW_{xh}} + {\\boldsymbol h_{t-1}W_{hh}} + {\\boldsymbol b_h})\n",
    "$$\n",
    "\n",
    "Here, $\\phi$ is an **[activation function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html)** of the hidden layer output. In contrast to [MLP](https://fedmug.github.io/kbtu-ml-book/mlp/layers.html), we preserve the hidden layer output $\\boldsymbol h_{t-1}$ from the previous time step. By introducing a new weight parameter $\\boldsymbol W_{hh}$ $\\in \\mathbb{R}^{h \\times h}$, we define how to use the hidden layer output from the previous time step in the current time step. Since the hidden state at the current time step uses the same definition as the previous time step, the computation involves *recurrence* which is why the model is called *recurrent neural network*.\n",
    "\n",
    "For time step $\\boldsymbol t$, the **output** of the output layer is computed similarly to [MLP](https://fedmug.github.io/kbtu-ml-book/mlp/forward_backward_pass.html):\n",
    "\n",
    "$$\n",
    "\\boldsymbol y_t = \\boldsymbol x_t\\boldsymbol W_{hy} + \\boldsymbol b_q \n",
    "$$\n",
    "\n",
    "Here, $\\boldsymbol y_t \\in \\mathbb{R}^{n \\times q}$ represents the output variable, $\\boldsymbol W_{hy} \\in \\mathbb{R}^{h \\times q}$ is the weight parameter, and $\\boldsymbol b_q \\in \\mathbb{R}^{1 \\times q}$ is the bias parameter. In the case of a classification problem, the softmax function can be applied to $\\boldsymbol y_t$ to compute the probability distribution of the output categories. As you can see, hidden state at the current time step, $\\boldsymbol h_t$, does not only participate in computing hidden state at next time step ${\\boldsymbol t+1}$, but is also used in output computation at current time step, $\\boldsymbol y_t$.\n",
    "\n",
    "```{note}\n",
    "RNNs consistently employ the same set of parameters, $\\boldsymbol W_{hx}, \\boldsymbol W_{hh}, \\boldsymbol W_{hy}, \\boldsymbol b_h, \\boldsymbol b_q$, across different time steps. This parameter reuse ensures that the computational cost of parameterization remains **constant**, irrespective of the number of time steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d5b4a-b220-45dc-b73b-3977b5c87fb2",
   "metadata": {},
   "source": [
    "\n",
    "## Training Vanilla RNNs\n",
    "\n",
    "Recurrent neural networks use **backpropagation through time (BPTT)**, which means forwarding through entire sequence to compute **losses**, then backwarding through entire sequence to compute **gradients** and update the weights accordingly. However, this becomes problematic if we want to train a sequence that is extremely long. In practice, an approximation called **truncated BPTT** is used, which is essentially running forward and backward through **chunks of the sequence** instead of the whole sequence.\n",
    "\n",
    "![RNN forward](images/truncated_bptt.png)\n",
    "\n",
    "To compute the loss $\\mathcal L$, the discrepancy between the output $\\boldsymbol y_t$ and the desired target $\\boldsymbol {\\widehat y_t}$ is evaluated by an objective function of a time step $\\boldsymbol t$ as:\n",
    "\n",
    "$ \\mathcal L_t(\\boldsymbol {x_t, y_t, W_h, W_y}) =  l({\\boldsymbol {\\widehat y_t}, y_t}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e2ce5-3771-41b5-bd36-4d6c74513913",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Calculation\n",
    "\n",
    "The gradient computation involves applying the chain rule to compute the gradients of the loss with respect to the parameters $\\boldsymbol w_h$ of objective function.\n",
    "\n",
    "$\n",
    "\\frac{\\partial L_t}{\\partial h_1} = \\frac{\\partial L_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_1} = \\frac{\\partial L_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_{t-1}} \\dots \\frac{\\partial h_2}{\\partial h_1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = \\left[1 - \\tanh^2(W_{hh}h_{t-1} + W_{hx}x_t + b)\\right] W_{hh}\n",
    "$\n",
    "\n",
    "Depending on the size of $\\boldsymbol W_{hh}$, the gradient can either vanish or explode over time:\n",
    "\n",
    "For matrix $\\boldsymbol W_{hh}$:\n",
    "  - If the largest singular value < 1: vanishing gradients.\n",
    "  - If the largest singular value > 1: exploding gradients.\n",
    "  \n",
    "To address the exploding gradient problem a technique called radient clipping is used. Gradient clipping imposes a constraint on the magnitude of the gradients, preventing them from exceeding a predefined threshold. If the L2 norm  of the gradients exceeds the threshold, it scales down all gradients proportionally to ensure that the overall norm is within the specified limit.\n",
    "\n",
    "$ \\nabla_{\\text{clipped}} = \\frac{clip\\_value}{\\max(clip\\_value, \\lVert \\nabla \\rVert}) \\cdot \\nabla $\n",
    "\n",
    "where:\n",
    "- $ \\nabla_{\\text{clipped}} $ is the clipped gradient vector\n",
    "- $ clip\\_value $ is the specified threshold\n",
    "- $ \\lVert \\nabla \\rVert $ is the L2 norm of the gradient vector.\n",
    "\n",
    "Considering more advanced RNN architectures like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) is a common and effective approach to address the vanishing gradient problem in traditional RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5749e0a-2392-4d56-96c1-de5e525b5fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
