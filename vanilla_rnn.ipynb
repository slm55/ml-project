{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a198c84-19a6-410e-bff0-49c4ccfe0549",
   "metadata": {},
   "source": [
    "#### Vanilla Recurrent Neural Networks (RNNs)\n",
    "\n",
    "\n",
    "Convolutional Neural Networks (CNNs) excel at tasks like image classification, where fixed-size inputs correspond to fixed-size outputs. However, they face challenges with variable-length sequences, such as time series, text sequences, and image sequences. Recurrent Neural Networks (RNNs) come to the forefront as a solution for processing sequential data.\n",
    "\n",
    "RNNs find application in diverse fields such as speech recognition, music generation, sentiment analysis, video processing, and text analysis and translation. Their ability to handle sequences makes them a powerful tool in capturing temporal dependencies.\n",
    "\n",
    "## Vanilla Recurrent Neural Network (RNN)\n",
    "\n",
    "The term \"Vanilla RNN\" is often used to refer to the basic form of recurrent neural network with a single hidden layer and without architectural enhancements. Vanilla RNN has a simple architecture consisting of an input layer, a recurrent hidden layer and an output layer.\n",
    "\n",
    "A basic RNN processes a time series of input data $X$ by estimating the output $y_t$ given the input vector $x_t$ and the hidden state vector $h_t$. The hidden state is updated at each time step. It acts as a memory of previous time steps allowing the network to capture sequential patterns.\n",
    "\n",
    "![RNN structure](images/rnn.png)\n",
    "\n",
    "For instance, consider a natural language processing task where $X$ is a sequence of words in a sentence, $x_t$ is the word at position $t$, and $y_t$ represents the predicted probability distribution over the vocabulary for the next word in the sequence. RNN learns from the context of previous words, using the hidden state to generate predictions for the next word in the sentence.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "![RNN forward](images/forward.png)\n",
    "\n",
    "Consider a minibatch of inputs $x_t \\in \\mathbb{R}^{n \\times d}$ at time step $t$. In this scenario, each row of $x_t$ corresponds to one example at time step $t$ within a minibatch of $n$ sequence examples. The weight parameter $W_{xh} \\in \\mathbb{R}^{d \\times h}$ and bias parameter $b_h$ are applied to the current input. Additionally, let $h_t \\in \\mathbb{R}^{n \\times h}$ denote the hidden layer output at time step $t$. In contrast to MLP, we preserve the hidden layer output $h_{t-1}$ from the previous time step. Introducing a new weight parameter $W_{hh} \\in \\mathbb{R}^{h \\times h}$, we define how to use the hidden layer output from the previous time step in the current time step. Specifically, the calculation of the hidden layer output at the current time step is determined by the input of the current time step, along with the hidden layer output of the previous time step:\n",
    "\n",
    "$ h_t = \\phi(x_tW_{xh} + h_{t-1}W_{hh} + b_h) $\n",
    "\n",
    "Since the hidden state at the current time step uses the same definition as the previous time step, the computation involves recurrence which is why the model is called recurrent neural network.\n",
    "\n",
    "For time step $t$, the output of the output layer is computed similarly to MLP:\n",
    "\n",
    "$ y_t = x_tW_{hy} + b_q $\n",
    "\n",
    "Here, $y_t \\in \\mathbb{R}^{n \\times q}$ represents the output variable, $W_{hy} \\in \\mathbb{R}^{h \\times q}$ is the weight parameter, and $b_q \\in \\mathbb{R}^{1 \\times q}$ is the bias parameter. In the case of a classification problem, the softmax function can be applied to $y_t$ to compute the probability distribution of the output categories. As you can see, hidden state at the current time step, $h_t$, does not only participate in computing hidden state at next time step $t+1$, but is also used in output computation at current time step, $y_t$.\n",
    "\n",
    "Note that RNNs consistently employ the same set of parameters, $W_{hx}, W_{hh}, W_{hy}, b_h, b_q$, across different time steps. This parameter reuse ensures that the computational cost of parameterization remains constant, irrespective of the number of time steps.\n",
    "\n",
    "\n",
    "## Training Vanilla RNNs\n",
    "\n",
    "Recurrent neural networks use backpropagation through time (BPTT), which means forwarding through entire sequence to compute losses, then backwarding through entire sequence to compute gradients and update the weights accordingly. However, this becomes problematic if we want to train a sequence that is extremely long. In practice, an approximation called truncated BPTT is used, which is essentially running forward and backward through chunks of the sequence instead of the whole sequence.\n",
    "\n",
    "![RNN forward](images/truncated_bptt.png)\n",
    "\n",
    "To compute the loss, the discrepancy between the output $y_t$ and the desired target $y'_t$ is evaluated by an objective function of a time step $t$ as:\n",
    "\n",
    "$ L_t(x_t, y_t, W_h, W_y) =  l(y'_t, y_t) $\n",
    "\n",
    "The gradient computation involves applying the chain rule to compute the gradients of the loss with respect to the parameters $w_h$ of objective function.\n",
    "\n",
    "$\n",
    "\\frac{\\partial L_t}{\\partial h_1} = \\frac{\\partial L_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_1} = \\frac{\\partial L_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_{t-1}} \\dots \\frac{\\partial h_2}{\\partial h_1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = \\left[1 - \\tanh^2(W_{hh}h_{t-1} + W_{hx}x_t + b)\\right] W_{hh}\n",
    "$\n",
    "\n",
    "Depending on the size of \\(W_{hh}\\), the gradient can either vanish or explode over time:\n",
    "\n",
    "For matrix $W_{hh}$:\n",
    "  - If the largest singular value < 1: vanishing gradients.\n",
    "  - If the largest singular value > 1: exploding gradients.\n",
    "  \n",
    "To address the exploding gradient problem a technique called radient clipping is used. Gradient clipping imposes a constraint on the magnitude of the gradients, preventing them from exceeding a predefined threshold. If the L2 norm  of the gradients exceeds the threshold, it scales down all gradients proportionally to ensure that the overall norm is within the specified limit.\n",
    "\n",
    "$ \\nabla_{\\text{clipped}} = \\frac{clip\\_value}{\\max(clip\\_value, \\lVert \\nabla \\rVert}) \\cdot \\nabla $\n",
    "\n",
    "where:\n",
    "- $ \\nabla_{\\text{clipped}} $ is the clipped gradient vector\n",
    "- $ clip\\_value $ is the specified threshold\n",
    "- $ \\lVert \\nabla \\rVert $ is the L2 norm of the gradient vector.\n",
    "\n",
    "Considering more advanced RNN architectures like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) is a common and effective approach to address the vanishing gradient problem in traditional RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e7081-f4de-41c9-9f60-852594fd852b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
